{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd, numpy as np, re, random, warnings, traceback\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Class imbalance rectification\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.decomposition import PCA, TruncatedSVD # imported later\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler, MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, RepeatedEditedNearestNeighbours\n",
    "from imblearn.over_sampling import ADASYN, SVMSMOTE\n",
    "\n",
    "# Optimization\n",
    "import optuna\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Dimensionality Reduction and remapping\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Model Eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = pd.read_csv('raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in some null values\n",
    "df['Priority'].fillna('No Priority Listed', inplace=True)\n",
    "df[\"General_Contractor\"].fillna('No General Contractor Listed', inplace=True)\n",
    "df['Remarks'].fillna('', inplace=True)\n",
    "df['EST'].fillna('No Estimator Listed', inplace=True)\n",
    "df['Addendum'].fillna('No Addendum', inplace=True)\n",
    "# Change to boolean\n",
    "df['Addendum'] = [1 if x != 'No Addendum' else 0 for x in df['Addendum']]\n",
    "\n",
    "# Strip whitespace, fill in null values\n",
    "df['Results'] = [str(x).strip() for x in df['Results']]\n",
    "df['Results'].fillna('No Result Listed', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the \"Description\" field of a ticket\n",
    "# Keep only alphanumeric values, strip extra space, keep it lowerspace\n",
    "def process_descriptions(txt):\n",
    "    return re.sub(\"[^a-zA-Z] +\", \"\", txt).lower().strip()   \n",
    "\n",
    "def get_traceback(e):\n",
    "    lines = traceback.format_exception(type(e), e, e.__traceback__)\n",
    "    return ''.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to use for modeling -- NOT stakeholder given\n",
    "model_cols = ['General_Contractor', 'Remarks', 'Addendum', 'EST', 'Results']\n",
    "# Split the dataframe\n",
    "res_df = df[model_cols]\n",
    "res_df = res_df.loc[res_df['Results'] != 'nan']\n",
    "\n",
    "classes = [1 if x == 'Awarded' else 0 for x in res_df['Results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create term counts and one hot encoding objects\n",
    "vec = CountVectorizer(decode_error='ignore', strip_accents='unicode', lowercase=True, stop_words='english')\n",
    "ohe = OneHotEncoder(drop='first', handle_unknown='infrequent_if_exist', sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>General_Contractor_Argo Systems</th>\n",
       "      <th>General_Contractor_CBP Constructors, LLC</th>\n",
       "      <th>General_Contractor_CYMA Builders of CM</th>\n",
       "      <th>General_Contractor_Consolidated Coatings</th>\n",
       "      <th>General_Contractor_Consolidated Medical Services</th>\n",
       "      <th>General_Contractor_Emaryland</th>\n",
       "      <th>General_Contractor_G &amp; I Drywall</th>\n",
       "      <th>General_Contractor_Herb Schafer Asphalt Pavement</th>\n",
       "      <th>General_Contractor_Jeffrey Brown Contracting, LLC</th>\n",
       "      <th>General_Contractor_KasCon, Inc</th>\n",
       "      <th>...</th>\n",
       "      <th>enviro</th>\n",
       "      <th>file</th>\n",
       "      <th>job</th>\n",
       "      <th>jobs</th>\n",
       "      <th>onlybid</th>\n",
       "      <th>phasingjob</th>\n",
       "      <th>sent</th>\n",
       "      <th>shifts</th>\n",
       "      <th>submitted</th>\n",
       "      <th>wk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    General_Contractor_Argo Systems  General_Contractor_CBP Constructors, LLC  \\\n",
       "0                               0.0                                       0.0   \n",
       "1                               0.0                                       0.0   \n",
       "2                               0.0                                       0.0   \n",
       "3                               0.0                                       0.0   \n",
       "4                               0.0                                       0.0   \n",
       "..                              ...                                       ...   \n",
       "71                              0.0                                       0.0   \n",
       "72                              0.0                                       0.0   \n",
       "73                              0.0                                       0.0   \n",
       "74                              0.0                                       0.0   \n",
       "75                              0.0                                       0.0   \n",
       "\n",
       "    General_Contractor_CYMA Builders of CM  \\\n",
       "0                                      0.0   \n",
       "1                                      0.0   \n",
       "2                                      0.0   \n",
       "3                                      0.0   \n",
       "4                                      0.0   \n",
       "..                                     ...   \n",
       "71                                     0.0   \n",
       "72                                     0.0   \n",
       "73                                     0.0   \n",
       "74                                     0.0   \n",
       "75                                     0.0   \n",
       "\n",
       "    General_Contractor_Consolidated Coatings  \\\n",
       "0                                        0.0   \n",
       "1                                        0.0   \n",
       "2                                        0.0   \n",
       "3                                        0.0   \n",
       "4                                        0.0   \n",
       "..                                       ...   \n",
       "71                                       0.0   \n",
       "72                                       0.0   \n",
       "73                                       0.0   \n",
       "74                                       0.0   \n",
       "75                                       0.0   \n",
       "\n",
       "    General_Contractor_Consolidated Medical Services  \\\n",
       "0                                                0.0   \n",
       "1                                                0.0   \n",
       "2                                                0.0   \n",
       "3                                                0.0   \n",
       "4                                                0.0   \n",
       "..                                               ...   \n",
       "71                                               0.0   \n",
       "72                                               0.0   \n",
       "73                                               0.0   \n",
       "74                                               0.0   \n",
       "75                                               0.0   \n",
       "\n",
       "    General_Contractor_Emaryland  General_Contractor_G & I Drywall  \\\n",
       "0                            0.0                               0.0   \n",
       "1                            0.0                               0.0   \n",
       "2                            0.0                               0.0   \n",
       "3                            0.0                               0.0   \n",
       "4                            0.0                               0.0   \n",
       "..                           ...                               ...   \n",
       "71                           0.0                               0.0   \n",
       "72                           0.0                               0.0   \n",
       "73                           0.0                               0.0   \n",
       "74                           0.0                               0.0   \n",
       "75                           0.0                               0.0   \n",
       "\n",
       "    General_Contractor_Herb Schafer Asphalt Pavement  \\\n",
       "0                                                0.0   \n",
       "1                                                0.0   \n",
       "2                                                0.0   \n",
       "3                                                0.0   \n",
       "4                                                0.0   \n",
       "..                                               ...   \n",
       "71                                               0.0   \n",
       "72                                               0.0   \n",
       "73                                               0.0   \n",
       "74                                               0.0   \n",
       "75                                               0.0   \n",
       "\n",
       "    General_Contractor_Jeffrey Brown Contracting, LLC  \\\n",
       "0                                                 0.0   \n",
       "1                                                 0.0   \n",
       "2                                                 0.0   \n",
       "3                                                 0.0   \n",
       "4                                                 0.0   \n",
       "..                                                ...   \n",
       "71                                                1.0   \n",
       "72                                                0.0   \n",
       "73                                                0.0   \n",
       "74                                                1.0   \n",
       "75                                                0.0   \n",
       "\n",
       "    General_Contractor_KasCon, Inc  ...  enviro  file  job  jobs  onlybid  \\\n",
       "0                              0.0  ...       0     0    0     0        0   \n",
       "1                              0.0  ...       0     0    0     0        0   \n",
       "2                              0.0  ...       0     0    0     0        0   \n",
       "3                              0.0  ...       0     0    0     0        0   \n",
       "4                              0.0  ...       0     0    0     0        0   \n",
       "..                             ...  ...     ...   ...  ...   ...      ...   \n",
       "71                             0.0  ...       0     0    1     0        0   \n",
       "72                             0.0  ...       0     0    1     0        0   \n",
       "73                             0.0  ...       0     0    0     0        0   \n",
       "74                             0.0  ...       0     0    0     0        0   \n",
       "75                             0.0  ...       0     0    0     0        0   \n",
       "\n",
       "    phasingjob  sent  shifts  submitted  wk  \n",
       "0            0     0       0          0   0  \n",
       "1            0     0       0          0   0  \n",
       "2            0     0       0          0   0  \n",
       "3            0     0       0          0   0  \n",
       "4            0     0       0          0   0  \n",
       "..         ...   ...     ...        ...  ..  \n",
       "71           0     0       0          0   0  \n",
       "72           0     0       0          0   0  \n",
       "73           0     0       0          0   0  \n",
       "74           0     0       0          0   0  \n",
       "75           0     0       0          0   0  \n",
       "\n",
       "[76 rows x 83 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the data\n",
    "cat_data = pd.DataFrame(data=ohe.fit_transform(res_df[[x for x in model_cols if x not in ['Results', 'Remarks']]]), columns=list(ohe.get_feature_names_out()))\n",
    "text_data = pd.DataFrame(data=vec.fit_transform(res_df['Remarks'].apply(process_descriptions)).todense(), columns=list(vec.get_feature_names_out()))\n",
    "modeling_df = pd.concat([cat_data, text_data], axis=1)\n",
    "modeling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(modeling_df, classes, stratify=classes, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization function using optuna\n",
    "def optimize_snow(trial, x_train, x_test, y_train, y_test):\n",
    "\n",
    "    n_row = x_train.shape[0]\n",
    "    n_col = x_train.shape[1]\n",
    "\n",
    "    # General Settings\n",
    "\n",
    "    # Class imbalance\n",
    "    undersample = trial.suggest_categorical(\"undersample\", [\"Yes\", \"No\"])\n",
    "    oversample = trial.suggest_categorical(\"oversample\", [\"Yes\", \"No\"])\n",
    "    \n",
    "    # Data Transformations / Dimensionality Reductions\n",
    "    data_type = trial.suggest_categorical(\"data_type\", [\"PCA\", \"UMAP\", \"RAW\"])    \n",
    "    \n",
    "    # Preprocessing\n",
    "    scaling = trial.suggest_categorical(\"scaling\", [\"None\", \"Standard\", \"Robust\"])\n",
    "    normalizing = trial.suggest_categorical(\"normalizing\", [\"Yes\", \"No\"])\n",
    "    minmax = trial.suggest_categorical(\"minmax\", [\"Yes\", \"No\"])\n",
    "    \n",
    "    algorithm_type = trial.suggest_categorical(\"algorithm_type\", ['classification', 'anomaly_detection'])\n",
    "    \n",
    "    # Important to use try/except because a lot of these params/combos will not work well together. The RL technique needs to learn this.\n",
    "    try:\n",
    "\n",
    "        # Dimensionality / Data Representation, tune those as well\n",
    "        if data_type == 'PCA':\n",
    "            \n",
    "            # Tune PCA\n",
    "            pca_n_components = trial.suggest_int(\"pca_n_components\", 1, n_col-1)\n",
    "            pca_whiten = trial.suggest_categorical(\"pca_whiten\", [True, False])\n",
    "            pca_svd_solver = trial.suggest_categorical(\"pca_svd_solver\", [\"auto\", \"full\", \"arpack\", \"randomized\"])\n",
    "\n",
    "            transformer = PCA(n_components=pca_n_components, whiten=pca_whiten, svd_solver=pca_svd_solver, random_state=8)\n",
    "            mm = MinMaxScaler(feature_range=(0,1))\n",
    "            x_train = transformer.fit_transform(mm.fit_transform(x_train))\n",
    "            x_test = transformer.transform(mm.transform(x_test))\n",
    "\n",
    "        elif data_type == 'UMAP':\n",
    "            \n",
    "            # Tune UMAP\n",
    "            umap_n_neighbors = trial.suggest_int(\"umap_n_neighbors\", 2, 75)\n",
    "            umap_min_dist = trial.suggest_float(\"umap_min_dist\", 0.0, 1.0)\n",
    "            umap_n_components = trial.suggest_int(\"umap_n_components\", 1, n_col - 1)\n",
    "            umap_metric = trial.suggest_categorical(\"umap_metric\", ['minkowski', 'cosine', 'correlation', 'euclidean', 'p', 'manhattan', \n",
    "                                                                    'infinity', 'nan_euclidean', 'dice', 'kulsinski', \n",
    "                                                                    'matching', 'jaccard', 'seuclidean', 'wminkowski', 'mahalanobis', \n",
    "                                                                    'rogerstanimoto', 'l1', 'sqeuclidean', 'sokalsneath', 'sokalmichener', \n",
    "                                                                    'l2', 'yule', 'canberra', 'cityblock', 'haversine', 'russellrao', \n",
    "                                                                    'hamming', 'braycurtis', 'chebyshev'])\n",
    "\n",
    "            transformer = UMAP(n_neighbors=umap_n_neighbors, min_dist=umap_min_dist, n_components=umap_n_components, metric=umap_metric, n_jobs=2)\n",
    "            mm = MinMaxScaler(feature_range=(0,1))\n",
    "            x_train = transformer.fit_transform(mm.fit_transform(x_train))\n",
    "            x_test = transformer.transform(mm.transform(x_test))        \n",
    "\n",
    "        # elif data_type == 'TSNE':\n",
    "            \n",
    "        #     # Tune TSNE\n",
    "        #     data_n_components = trial.suggest_int(\"data_n_components\", 1, x_train.shape[1] - 1)\n",
    "        #     early_exaggeration = trial.suggest_float(\"early_exaggeration\", 1.0, 25.0)\n",
    "        #     perplexity = trial.suggest_float(\"perplexity\", 5.0, 50.0)\n",
    "        #     data_n_iter = trial.suggest_int(\"data_n_iter\", 250, 5000)\n",
    "        #     data_n_iter_without_progress = trial.suggest_int(\"data_n_iter_without_progress\", 300, 5000)\n",
    "        #     min_grad_norm = trial.suggest_float(\"min_grad_norm\", .0000001, 0.999999)\n",
    "        #     data_init = trial.suggest_categorical(\"data_init\", ['random', 'pca'])\n",
    "\n",
    "        #     transformer = TSNE(n_components=data_n_components, n_iter=data_n_iter, early_exaggeration=early_exaggeration,\n",
    "        #                 perplexity=perplexity, n_iter_without_progress=data_n_iter_without_progress, n_jobs=2,\n",
    "        #                 min_grad_norm=min_grad_norm, init=data_init)\n",
    "        #     mm = MinMaxScaler(feature_range=(0,1))\n",
    "        #     x_train = transformer.fit_transform(mm.fit_transform(x_train))\n",
    "        #     x_test = transformer.transform(mm.transform(x_test))  \n",
    "\n",
    "        else:\n",
    "            x_train = x_train.to_numpy()\n",
    "            x_test = x_test.to_numpy()\n",
    "\n",
    "        # Scale\n",
    "        if scaling == 'Standard':\n",
    "            ss = StandardScaler()\n",
    "            x_train = ss.fit_transform(x_train)\n",
    "            x_test = ss.transform(x_test)\n",
    "        elif scaling == 'Robust':\n",
    "            rb = RobustScaler()\n",
    "            x_train = rb.fit_transform(x_train)\n",
    "            x_test = rb.transform(x_test)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Normalize\n",
    "        if normalizing == 'Yes':\n",
    "            nm = Normalizer()\n",
    "            x_train = nm.fit_transform(x_train)\n",
    "            x_test = nm.transform(x_test)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Minmax Scaling\n",
    "        if minmax == 'Yes':\n",
    "            mm = MinMaxScaler(feature_range=(-1,1))\n",
    "            x_train = mm.fit_transform(x_train)\n",
    "            x_test = mm.transform(x_test)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Note -- we will need to add support for multiclass eventually\n",
    "        \n",
    "        # Undersample the majority class\n",
    "        if undersample == 'Yes':\n",
    "            usampler = trial.suggest_categorical(\"usampler\", [\"Random\", \"Tomek\", \"Repeated\"])\n",
    "\n",
    "            if usampler == 'Random':\n",
    "                random_replacement = trial.suggest_categorical(\"random_replacement\", [True, False])\n",
    "\n",
    "                under_sampler = RandomUnderSampler(random_state=8, sampling_strategy='majority', replacement=random_replacement)\n",
    "                x_train, y_train = under_sampler.fit_resample(x_train, y_train)\n",
    "\n",
    "            elif usampler == 'Tomek':\n",
    "                under_sampler = TomekLinks(sampling_strategy='majority', n_jobs=4)\n",
    "                x_train, y_train = under_sampler.fit_resample(x_train, y_train)\n",
    "            elif usampler == 'Repeated':\n",
    "                repeated_n_neighbors = trial.suggest_int(\"repeated_n_neighbors\", 2, 100)\n",
    "                repeated_max_iter = trial.suggest_int(\"repeated_max_iter\", 2, 1000)\n",
    "                repeated_kind_sel = trial.suggest_categorical(\"repeated_kind_sel\", ['all', 'mode'])\n",
    "                \n",
    "                under_sampler = RepeatedEditedNearestNeighbours(sampling_strategy='majority', n_jobs=4, n_neighbors=repeated_n_neighbors, \n",
    "                                                                max_iter=repeated_max_iter, kind_sel=repeated_kind_sel)\n",
    "                x_train, y_train = under_sampler.fit_resample(x_train, y_train)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Oversample the minority class\n",
    "        if oversample == 'Yes':\n",
    "            osampler = trial.suggest_categorical(\"osampler\", [\"ADASYN\", \"SVMSMOTE\"])\n",
    "\n",
    "            if osampler == 'ADASYN':\n",
    "                oversample_n_neighbors = trial.suggest_int(\"oversample_n_neighbors\", 2, 100)\n",
    "                \n",
    "                over_sampler = ADASYN(sampling_strategy='minority', random_state=8, n_jobs=4, n_neighbors=oversample_n_neighbors)\n",
    "                x_train, y_train = over_sampler.fit_resample(x_train, y_train)\n",
    "            elif osampler == 'SVMSMOTE':\n",
    "                svmsmote_k_neighbors = trial.suggest_int(\"svmsmote_k_neighbors\", 2, 100)\n",
    "                svmsmote_m_neighbors = trial.suggest_int(\"svmsmote_m_neighbors\", 2, 100)\n",
    "                svmsmote_out_step = trial.suggest_float(\"svmsmote_out_step\", 0.01, 0.99)\n",
    "\n",
    "                svmsmote_svm_c = trial.suggest_float(\"svmsmote_svm_c\", 0.01, 1.0)\n",
    "                svmsmote_svm_kernel = trial.suggest_categorical(\"svmsmote_svm_kernel\", ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "                svmsmote_svm_gamma = trial.suggest_categorical(\"svmsmote_svm_gamma\", ['auto', 'scale'])\n",
    "                svmsmote_svm_shrinking = trial.suggest_categorical(\"svmsmote_svm_shrinking\", [True, False])\n",
    "                svmsmote_svm_max_iter = trial.suggest_int(\"svmsmote_svm_max_iter\", 2, 1000)\n",
    "                svmsmote_svm_decision_function = trial.suggest_categorical(\"svmsmote_svm_decision_function\", ['ovo', 'ovr'])\n",
    "                svm_obj = SVC(C=svmsmote_svm_c, kernel=svmsmote_svm_kernel, gamma=svmsmote_svm_gamma, shrinking=svmsmote_svm_shrinking,\n",
    "                            max_iter=svmsmote_svm_max_iter, decision_function_shape=svmsmote_svm_decision_function, class_weight='balanced', cache_size=750, random_state=8)\n",
    "\n",
    "                over_sampler = SVMSMOTE(k_neighbors=svmsmote_k_neighbors, m_neighbors=svmsmote_m_neighbors, sampling_strategy='minority', \n",
    "                                        out_step=svmsmote_out_step, random_state=8, n_jobs=4, svm_estimator=svm_obj)\n",
    "                x_train, y_train = over_sampler.fit_resample(x_train, y_train)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # run through some classification algorithms\n",
    "        # obviously, we should expand the parameter selection & the algorithms list eventually\n",
    "        if algorithm_type == 'classification':\n",
    "            c_algorithm = trial.suggest_categorical(\"c_algorithm\", [\"Logistic Regression\", \"Random Forest\", \"Gradient Boosting\", \"Extra Trees\", \"Bernoulli\", \"Multinomial\", \"XGBoost\", \"LightGBM\"])\n",
    "\n",
    "            if c_algorithm == 'XGBoost':\n",
    "                xgb_max_depth = trial.suggest_int(\"xgb_max_depth\", 2, 50)\n",
    "                xgb_min_child_weight = trial.suggest_int(\"xgb_min_child_weight\", 0, 50)\n",
    "                xgb_gamma = trial.suggest_int(\"xgb_gamma\", 0, 100)\n",
    "                xgb_subsample = trial.suggest_float(\"xgb_subsample\", 0, 1)\n",
    "                xgb_max_delta_step = trial.suggest_int(\"xgb_max_delta_step\", 0, 10)\n",
    "\n",
    "                algo = XGBClassifier(max_depth=xgb_max_depth, min_child_weight=xgb_min_child_weight, gamma=xgb_gamma, subsample=xgb_subsample, max_delta_step=xgb_max_delta_step)\n",
    "            elif c_algorithm == 'LightGBM':\n",
    "                lgb_lambda_l1 = trial.suggest_loguniform('lgb_lambda_l1', 1e-8, 10.0)\n",
    "                lgb_lambda_l2 = trial.suggest_loguniform('lgb_lambda_l2', 1e-8, 10.0)\n",
    "                lgb_num_leaves = trial.suggest_int('lgb_num_leaves', 2, 256)\n",
    "                lgb_feature_fraction = trial.suggest_uniform('lgb_feature_fraction', 0.4, 1.0)\n",
    "                lgb_bagging_fraction = trial.suggest_uniform('lgb_bagging_fraction', 0.4, 1.0)\n",
    "                lgb_bagging_freq = trial.suggest_int('lgb_bagging_freq', 1, 7)\n",
    "                lgb_min_child_samples = trial.suggest_int('lgb_min_child_samples', 5, 100)     \n",
    "\n",
    "                algo = LGBMClassifier(lambda_l1=lgb_lambda_l1, lambda_l2=lgb_lambda_l2, num_leaves=lgb_num_leaves, feature_fraction=lgb_feature_fraction, \n",
    "                                      bagging_fraction=lgb_bagging_fraction, bagging_freq=lgb_bagging_freq, min_child_samples=lgb_min_child_samples)\n",
    "            elif c_algorithm == 'Logistic Regression':\n",
    "                lr_penalty = trial.suggest_categorical('lr_penalty', ['none', 'l2', 'l1', 'elasticnet'])\n",
    "                lr_dual = trial.suggest_categorical('lr_dual', [True, False])\n",
    "                # tol = trial.suggest_float('')\n",
    "                lr_c = trial.suggest_float('lr_c', 0.01, 1.0)\n",
    "                lr_fit_intercept = trial.suggest_categorical('lr_fit_intercept', [True, False])\n",
    "                lr_solver = trial.suggest_categorical('lr_solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])\n",
    "                lr_max_iter = trial.suggest_int('lr_max_iter', 2, 1000)\n",
    "                lr_multi_class = trial.suggest_categorical('lr_multi_class', ['auto', 'ovr', 'multinomial'])\n",
    "\n",
    "                algo = LogisticRegression(penalty=lr_penalty, dual=lr_dual, C=lr_c, fit_intercept=lr_fit_intercept, class_weight='balanced', random_state=8, n_jobs=4,\n",
    "                                          solver=lr_solver, max_iter=lr_max_iter, multi_class=lr_multi_class)\n",
    "            elif c_algorithm == 'Random Forest':\n",
    "                rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 2, 1000)\n",
    "                rf_criterion = trial.suggest_categorical(\"rf_criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
    "                rf_min_samples_split = trial.suggest_float(\"rf_min_samples_split\", 0.01, 1.0)\n",
    "                rf_min_samples_leaf = trial.suggest_float(\"rf_min_samples_leaf\", 0.01, 1.0)\n",
    "                rf_min_weight_fraction_leaf = trial.suggest_float(\"rf_min_weight_fraction_leaf\", 0.0, 0.5)\n",
    "                rf_max_features = trial.suggest_categorical(\"rf_max_features\", [\"sqrt\", \"log2\", None])\n",
    "                rf_bootstrap = trial.suggest_categorical(\"rf_bootstrap\", [True, False])\n",
    "                rf_oob_score = trial.suggest_categorical(\"rf_oob_score\", [True, False])\n",
    "                rf_class_weight = trial.suggest_categorical(\"rf_class_weight\", ['balanced', 'balanced_subsample'])\n",
    "                \n",
    "                algo = RandomForestClassifier(n_estimators=rf_n_estimators, criterion=rf_criterion, min_samples_split=rf_min_samples_split,\n",
    "                                              min_samples_leaf=rf_min_samples_leaf, max_features=rf_max_features, min_weight_fraction_leaf=rf_min_weight_fraction_leaf,\n",
    "                                              bootstrap=rf_bootstrap, oob_score=rf_oob_score, class_weight=rf_class_weight, n_jobs=4, random_state=8)\n",
    "            elif c_algorithm == 'Extra Trees':\n",
    "                et_n_estimators = trial.suggest_int(\"et_n_estimators\", 2, 1000)\n",
    "                et_criterion = trial.suggest_categorical(\"et_criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
    "                et_min_samples_split = trial.suggest_float(\"et_min_samples_split\", 0.01, 1.0)\n",
    "                et_min_samples_leaf = trial.suggest_float(\"et_min_samples_leaf\", 0.01, 1.0)\n",
    "                et_min_weight_fraction_leaf = trial.suggest_float(\"et_min_weight_fraction_leaf\", 0.0, 0.5)\n",
    "                et_max_features = trial.suggest_categorical(\"et_max_features\", [\"sqrt\", \"log2\", None])\n",
    "                et_bootstrap = trial.suggest_categorical(\"et_bootstrap\", [True, False])\n",
    "                et_oob_score = trial.suggest_categorical(\"et_oob_score\", [True, False])\n",
    "                et_class_weight = trial.suggest_categorical(\"et_class_weight\", ['balanced', 'balanced_subsample'])\n",
    "                \n",
    "                algo = ExtraTreesClassifier(n_estimators=et_n_estimators, criterion=et_criterion, min_samples_split=et_min_samples_split,\n",
    "                                              min_samples_leaf=et_min_samples_leaf, max_features=et_max_features, min_weight_fraction_leaf=et_min_weight_fraction_leaf,\n",
    "                                              bootstrap=et_bootstrap, oob_score=et_oob_score, class_weight=et_class_weight, n_jobs=4, random_state=8)                                              \n",
    "            elif c_algorithm == 'Gradient Boosting':\n",
    "                gb_loss = trial.suggest_categorical(\"gb_loss\", ['log_loss', 'deviance', 'exponential'])\n",
    "                gb_n_estimators = trial.suggest_int(\"gb_n_estimators\", 2, 1000)\n",
    "                gb_criterion = trial.suggest_categorical(\"gb_criterion\", ['friedman_mse', 'squared_error', 'mse'])\n",
    "                gb_min_samples_split = trial.suggest_float(\"gb_min_samples_split\", 0.01, 1.0)\n",
    "                gb_min_samples_leaf = trial.suggest_float(\"gb_min_samples_leaf\", 0.01, 1.0)\n",
    "                gb_min_weight_fraction_leaf = trial.suggest_float(\"gb_min_weight_fraction_leaf\", 0.0, 1.0)\n",
    "                gb_max_features = trial.suggest_categorical(\"gb_max_features\", [\"sqrt\", \"log2\", None])\n",
    "\n",
    "                algo = GradientBoostingClassifier(loss=gb_loss, n_estimators=gb_n_estimators, criterion=gb_criterion, min_samples_split=gb_min_samples_split,\n",
    "                                                  min_samples_leaf=gb_min_samples_leaf, min_weight_fraction_leaf=gb_min_weight_fraction_leaf, max_features=gb_max_features, \n",
    "                                                  random_state=8)\n",
    "            elif c_algorithm == 'Bernoulli':\n",
    "                bern_alpha = trial.suggest_float(\"bern_alpha\", 0.0, 2.0)\n",
    "                bern_binarize = trial.suggest_float(\"bern_binarize\", 0.0, 1.0)\n",
    "                bern_fit_prior = trial.suggest_categorical(\"bern_fit_prior\", [True, False])\n",
    "                \n",
    "                algo = BernoulliNB(alpha=bern_alpha, binarize=bern_binarize, fit_prior=bern_fit_prior)\n",
    "\n",
    "            elif c_algorithm == 'MultinomialNB':\n",
    "                mn_alpha = trial.suggest_float(\"mn_alpha\", 0.0, 2.0)\n",
    "                mn_fit_prior = trial.suggest_categorical(\"mn_fit_prior\", [True, False])                \n",
    "\n",
    "                algo = MultinomialNB(alpha=mn_alpha, fit_prior=mn_fit_prior)\n",
    "\n",
    "            else:\n",
    "                c_algorithm = \"None\"\n",
    "\n",
    "        elif algorithm_type == 'anomaly_detection':\n",
    "            # a_algorithm = trial.suggest_categorical(\"a_algorithm\", ['ABOD', 'COPOD', 'SOS', 'Sampling', 'GMM', 'MCD', 'CD', 'OCSVM', 'LMDD', 'LOF', \n",
    "            #                                                     'COF', 'CBLOF', 'HBOS', 'SOD', 'IForest', 'INNE', 'XGBOD', 'LODA', 'SUOD', \n",
    "            #                                                     'AutoEncoder', 'VAE', 'SO_GAAL', 'MO_GAAL', 'DeepSVDD', 'AnoGAN', 'RGraph', 'LUNAR'])\n",
    "            a_algorithm = trial.suggest_categorical(\"a_algorithm\", ['ABOD', 'COPOD', 'SOS', 'Sampling', 'GMM', 'MCD', 'CD', 'OCSVM', 'LMDD', 'LOF'])                                                              \n",
    "            if a_algorithm == 'ABOD':\n",
    "                abod_contamination = trial.suggest_float('abod_contamination', 0, 0.5)\n",
    "                abod_n_neighbors = trial.suggest_int('abod_n_neighbors', 1, 100)\n",
    "\n",
    "                algo = ABOD(contamination=abod_contamination, n_neighbors=abod_n_neighbors, method='fast')\n",
    "            elif a_algorithm == 'COPOD':\n",
    "                copod_contamination = trial.suggest_float('copod_contamination', 0, 0.5)\n",
    "                copod_n_neighbors = trial.suggest_int('copod_n_neighbors', 1, 100)\n",
    "\n",
    "                algo = COPOD(contamination=copod_contamination, n_neighbors=copod_n_neighbors)\n",
    "            elif a_algorithm == 'SOS':\n",
    "                sos_contamination = trial.suggest_float('sos_contamination', 0, 0.5)\n",
    "                sos_perplexity = trial.suggest_float(\"sos_perplexity\", 1.0, 100.0)                \n",
    "                sos_metric = trial.suggest_categorical('sos_metric', ['braycurtis', 'canberra', 'chebyshev', 'correlation', \n",
    "                                                              'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', \n",
    "                                                              'matching', 'minkowski', 'rogerstanimoto', 'russellrao', \n",
    "                                                              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', \n",
    "                                                              'yule', 'euclidean'])\n",
    "\n",
    "                algo = SOS(contamination=sos_contamination, perplexity=sos_perplexity, metric=sos_metric)\n",
    "            elif a_algorithm == 'Sampling':\n",
    "                sampling_contamination = trial.suggest_float('sampling_contamination', 0, 0.5)\n",
    "                sampling_subset_size = trial.suggest_float('sampling_subset_size', 0, 1.0)                \n",
    "                sampling_metric = trial.suggest_categorical('sampling_metric', ['braycurtis', 'canberra', 'chebyshev', 'correlation', \n",
    "                                                              'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', \n",
    "                                                              'matching', 'minkowski', 'rogerstanimoto', 'russellrao', \n",
    "                                                              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', \n",
    "                                                              'yule', 'euclidean', 'cityblock', 'cosine', 'euclidean', 'l1', 'l2', \n",
    "                                                              'manhattan'])\n",
    "\n",
    "                algo = Sampling(contamination=sampling_contamination, subset_size=sampling_subset_size, metric=sampling_metric)\n",
    "\n",
    "            elif a_algorithm == 'GMM':\n",
    "                gmm_n_components = trial.suggest_int('gmm_n_components', 1, 100)\n",
    "                gmm_covariance_type = trial.suggest_categorical('gmm_covariance_type', ['full', 'tied', 'diag', 'spherical'])\n",
    "                gmm_tol = trial.suggest_float('gmm_tol', 1e-1, 1e-10)\n",
    "                gmm_reg_covar = trial.suggest_float('gmm_reg_covar', 1e-1, 1e-10)\n",
    "                gmm_max_iter = trial.suggest_int('gmm_max_iter', 2, 1000)\n",
    "                gmm_init_params = trial.suggest_categorical('gmm_init_params', ['kmeans', 'random'])\n",
    "                gmm_contamination = trial.suggest_float('gmm_contamination', 0, 0.5)\n",
    "\n",
    "                algo = GMM(n_components=gmm_n_components, gmm_covariance_type=gmm_covariance_type, tol=gmm_tol, reg_covar=gmm_reg_covar,\n",
    "                            max_iter=gmm_max_iter, init_params=gmm_init_params, contamination=gmm_contamination, random_state=8)\n",
    "            elif a_algorithm == 'MCD':\n",
    "                mcd_store_precision = trial.suggest_categorical('mcd_store_precision', [True, False])\n",
    "                mcd_assume_centered = trial.suggest_categorical('mcd_assume_centered', [True, False])\n",
    "                mcd_contamination = trial.suggest_float('mcd_contamination', 0, 0.5)\n",
    "\n",
    "                algo = MCD(contamination=mcd_contamination, store_precision=mcd_store_precision, assume_centered=mcd_assume_centered, random_state=8)\n",
    "\n",
    "            elif a_algorithm == 'CD':\n",
    "                cd_n_components = trial.suggest_int('cd_n_components', 1, 100)\n",
    "                cd_whiten = trial.suggest_categorical('cd_whiten', [True, False])\n",
    "                cd_rule_of_thumb = trial.suggest_cateogorical('cd_rule_of_thumb', [True, False])\n",
    "                cd_contamination = trial.suggest_float('cd_contamination', 0, 0.5)\n",
    "\n",
    "                algo = CD(contamination=cd_contamination, n_components=cd_n_components, whiten=cd_whiten, rule_of_thumb=cd_rule_of_thumb)\n",
    "            elif a_algorithm == 'OCSVM':\n",
    "                ocsvm_svm_kernel = trial.suggest_categorical(\"ocsvm_svm_kernel\", ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "                ocsvm_svm_gamma = trial.suggest_categorical(\"ocsvm_svm_gamma\", ['auto', 'scale'])\n",
    "                ocsvm_svm_shrinking = trial.suggest_categorical(\"ocsvm_svm_gamma\", [True, False])\n",
    "                ocsvm_svm_max_iter = trial.suggest_int(\"ocsvm_svm_max_iter\", 2, 1000)\n",
    "                ocsvm_contamination = trial.suggest_float('ocsvm_contamination', 0, 0.5)\n",
    "\n",
    "                algo = OCSVM(kernel=ocsvm_svm_kernel, gamma=ocsvm_svm_gamma, shrinking=ocsvm_svm_shrinking, max_iter=ocsvm_svm_max_iter, \n",
    "                              cache_size=750, random_state=8, contamination=ocsvm_contamination)\n",
    "            elif a_algorithm == 'LMDD':\n",
    "                lmdd_contamination = trial.suggest_float('lmdd_contamination', 0, 0.5)\n",
    "                lmdd_n_iter = trial.suggest_int('lmdd_n_iter', 2, 1000)\n",
    "                lmdd_dis_measure = trial.suggest_categorical('lmdd_dis_measure', ['aad', 'var', 'iqr'])\n",
    "\n",
    "                algo = LMDD(contamination=lmdd_contamination, n_iter=lmdd_n_iter, dis_measure=lmdd_dis_measure, random_state=8)\n",
    "\n",
    "            elif a_algorithm == 'LOF':\n",
    "                lof_contamination = trial.suggest_float('lof_contamination', 0, 0.5)\n",
    "                lof_n_neighbors = trial.suggest_int('lof_n_neighbors', 2, 100)\n",
    "                lof_algorithm = trial.suggest_categorical('lof_algorithm', ['ball_tree', 'kd_tree', 'brute', 'auto'])\n",
    "                lof_metric = trial.suggest_categorical('lof_metric', ['braycurtis', 'canberra', 'chebyshev', 'correlation', \n",
    "                                                              'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', \n",
    "                                                              'matching', 'minkowski', 'rogerstanimoto', 'russellrao', \n",
    "                                                              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', \n",
    "                                                              'yule', 'euclidean', 'cityblock', 'cosine', 'euclidean', 'l1', 'l2', \n",
    "                                                              'manhattan'])\n",
    "                \n",
    "                algo = LOF(contamination=lof_contamination, n_neighbors=lof_n_neighbors, algorithm=lof_algorithm, n_jobs=4, metric=lof_metric)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            algo = XGBClassifier()\n",
    "        \n",
    "        algo.fit(x_train, y_train)\n",
    "        predictions = algo.predict(x_test)\n",
    "        score = balanced_accuracy_score(y_test, predictions)\n",
    "\n",
    "        del algo, predictions, x_train, x_test, y_train, y_test\n",
    "        \n",
    "\n",
    "        return score\n",
    "    \n",
    "    # We need better logging with full tracebacks for better debugging\n",
    "    except Exception as E:\n",
    "        print(\"======================================LOGGING======================================\")\n",
    "        print(\"Error is: \")\n",
    "        # print(get_traceback(E))\n",
    "        traceback.print_exc()\n",
    "        print(\"======================================LOGGING======================================\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        return -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-30 00:23:46,817]\u001b[0m A new study created in memory with name: no-name-4491efff-6024-40a9-8496-cd303ba90eb7\u001b[0m\n",
      "\u001b[32m[I 2022-11-30 00:23:47,035]\u001b[0m Trial 0 finished with value: -100.0 and parameters: {'undersample': 'Yes', 'oversample': 'No', 'data_type': 'PCA', 'scaling': 'Standard', 'normalizing': 'No', 'minmax': 'No', 'algorithm_type': 'classification', 'c_algorithm': 'Extra Trees', 'data_n_components': 25, 'whiten': False, 'svd_solver': 'full', 'usampler': 'Random', 'replacement': True, 'n_estimators': 670, 'criterion': 'gini', 'min_samples_split': 0.6751205504194089, 'min_samples_leaf': 0.6770612993475804, 'min_weight_fraction_leaf': 0.8202993254506294, 'max_features': 'log2', 'bootstrap': False, 'oob_score': False, 'class_weight': 'balanced'}. Best is trial 0 with value: -100.0.\u001b[0m\n",
      "\u001b[32m[I 2022-11-30 00:23:47,039]\u001b[0m Trial 1 finished with value: -100.0 and parameters: {'undersample': 'No', 'oversample': 'No', 'data_type': 'RAW', 'scaling': 'Standard', 'normalizing': 'Yes', 'minmax': 'No', 'algorithm_type': 'classification', 'c_algorithm': 'Gradient Boosting', 'loss': 'exponential', 'n_estimators': 476}. Best is trial 0 with value: -100.0.\u001b[0m\n",
      "\u001b[32m[I 2022-11-30 00:23:47,054]\u001b[0m Trial 2 finished with value: -100.0 and parameters: {'undersample': 'No', 'oversample': 'Yes', 'data_type': 'RAW', 'scaling': 'Robust', 'normalizing': 'Yes', 'minmax': 'No', 'algorithm_type': 'classification', 'c_algorithm': 'XGBoost', 'osampler': 'SVMSMOTE', 'oversample_k_neighbors': 47, 'oversample_m_neighbors': 62, 'oversample_out_step': 0.8841320875836304, 'oversample_svm_c': 0.9679596527678364, 'oversample_svm_kernel': 'rbf', 'oversample_svm_gamma': 'auto'}. Best is trial 0 with value: -100.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================LOGGING======================================\n",
      "Error is: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kelly\\AppData\\Local\\Temp\\ipykernel_17300\\885668637.py\", line 253, in optimize_dabbco\n",
      "    algo.fit(x_train, y_train)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 476, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"C:\\Users\\Kelly\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\pool.py\", line 771, in get\n",
      "    raise self._value\n",
      "  File \"C:\\Users\\Kelly\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 191, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 286, in fit\n",
      "    check_scalar(\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1489, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: min_weight_fraction_leaf == 0.8202993254506294, must be <= 0.5.\n",
      "\n",
      "======================================LOGGING======================================\n",
      "\n",
      "\n",
      "======================================LOGGING======================================\n",
      "Error is: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kelly\\AppData\\Local\\Temp\\ipykernel_17300\\885668637.py\", line 228, in optimize_dabbco\n",
      "    criterion = trial.suggest_categorical(\"criterion\", ['friedman_mse', 'squared_error', 'mse'])\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\optuna\\trial\\_trial.py\", line 505, in suggest_categorical\n",
      "    return self._suggest(name, CategoricalDistribution(choices=choices))\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\optuna\\trial\\_trial.py\", line 724, in _suggest\n",
      "    storage.set_trial_param(trial_id, name, param_value_in_internal_repr, distribution)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\optuna\\storages\\_in_memory.py\", line 262, in set_trial_param\n",
      "    distributions.check_distribution_compatibility(\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\optuna\\distributions.py\", line 530, in check_distribution_compatibility\n",
      "    raise ValueError(\n",
      "ValueError: CategoricalDistribution does not support dynamic value space.\n",
      "\n",
      "======================================LOGGING======================================\n",
      "\n",
      "\n",
      "======================================LOGGING======================================\n",
      "Error is: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kelly\\AppData\\Local\\Temp\\ipykernel_17300\\885668637.py\", line 151, in optimize_dabbco\n",
      "    oversample_svm_shrinking = trial.suggest_categorical(\"oversample_svm_gamma\", [True, False])\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\optuna\\trial\\_trial.py\", line 505, in suggest_categorical\n",
      "    return self._suggest(name, CategoricalDistribution(choices=choices))\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\optuna\\trial\\_trial.py\", line 708, in _suggest\n",
      "    distributions.check_distribution_compatibility(trial.distributions[name], distribution)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\optuna\\distributions.py\", line 530, in check_distribution_compatibility\n",
      "    raise ValueError(\n",
      "ValueError: CategoricalDistribution does not support dynamic value space.\n",
      "\n",
      "======================================LOGGING======================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-30 00:23:47,262]\u001b[0m Trial 3 finished with value: 0.5 and parameters: {'undersample': 'Yes', 'oversample': 'No', 'data_type': 'RAW', 'scaling': 'Robust', 'normalizing': 'No', 'minmax': 'Yes', 'algorithm_type': 'classification', 'c_algorithm': 'XGBoost', 'usampler': 'Repeated', 'undersample_n_neighbors': 12, 'undersample_max_iter': 50, 'undersample_kind_sel': 'all', 'max_depth': 23, 'min_child_weight': 45, 'gamma': 93, 'subsample': 0.9106728411908478, 'max_delta_step': 6}. Best is trial 3 with value: 0.5.\u001b[0m\n",
      "\u001b[32m[I 2022-11-30 00:23:47,268]\u001b[0m Trial 4 finished with value: -100.0 and parameters: {'undersample': 'Yes', 'oversample': 'Yes', 'data_type': 'RAW', 'scaling': 'Standard', 'normalizing': 'Yes', 'minmax': 'Yes', 'algorithm_type': 'classification', 'c_algorithm': 'Multinomial', 'usampler': 'Random', 'replacement': False}. Best is trial 3 with value: 0.5.\u001b[0m\n",
      "\u001b[32m[I 2022-11-30 00:23:47,288]\u001b[0m Trial 5 finished with value: -100.0 and parameters: {'undersample': 'Yes', 'oversample': 'No', 'data_type': 'PCA', 'scaling': 'Robust', 'normalizing': 'No', 'minmax': 'No', 'algorithm_type': 'classification', 'c_algorithm': 'Extra Trees', 'data_n_components': 47, 'whiten': False, 'svd_solver': 'full', 'usampler': 'Repeated', 'undersample_n_neighbors': 86, 'undersample_max_iter': 81, 'undersample_kind_sel': 'all'}. Best is trial 3 with value: 0.5.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================LOGGING======================================\n",
      "Error is: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kelly\\AppData\\Local\\Temp\\ipykernel_17300\\885668637.py\", line 253, in optimize_dabbco\n",
      "    algo.fit(x_train, y_train)\n",
      "UnboundLocalError: local variable 'algo' referenced before assignment\n",
      "\n",
      "======================================LOGGING======================================\n",
      "\n",
      "\n",
      "======================================LOGGING======================================\n",
      "Error is: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kelly\\AppData\\Local\\Temp\\ipykernel_17300\\885668637.py\", line 130, in optimize_dabbco\n",
      "    x_train, y_train = under_sampler.fit_resample(x_train, y_train)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\imblearn\\base.py\", line 83, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_edited_nearest_neighbours.py\", line 334, in _fit_resample\n",
      "    X_enn, y_enn = self.enn_.fit_resample(X_, y_)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\imblearn\\base.py\", line 83, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_edited_nearest_neighbours.py\", line 155, in _fit_resample\n",
      "    nnhood_idx = self.nn_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 749, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples,  but n_samples = 60, n_neighbors = 87\n",
      "\n",
      "======================================LOGGING======================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-30 00:23:51,789]\u001b[0m Trial 6 finished with value: -100.0 and parameters: {'undersample': 'Yes', 'oversample': 'No', 'data_type': 'UMAP', 'scaling': 'Standard', 'normalizing': 'No', 'minmax': 'Yes', 'algorithm_type': 'classification', 'c_algorithm': 'Multinomial', 'data_n_neighbors': 60, 'min_dist': 0.2577743868193004, 'data_n_components': 63, 'data_metric': 'yule'}. Best is trial 3 with value: 0.5.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================LOGGING======================================\n",
      "Error is: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kelly\\AppData\\Local\\Temp\\ipykernel_17300\\885668637.py\", line 56, in optimize_dabbco\n",
      "    x_train = transformer.fit_transform(mm.fit_transform(x_train))\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\umap\\umap_.py\", line 2772, in fit_transform\n",
      "    self.fit(X, y)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\umap\\umap_.py\", line 2684, in fit\n",
      "    self.embedding_, aux_data = self._fit_embed_data(\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\umap\\umap_.py\", line 2717, in _fit_embed_data\n",
      "    return simplicial_set_embedding(\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\umap\\umap_.py\", line 1078, in simplicial_set_embedding\n",
      "    initialisation = spectral_layout(\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\umap\\spectral.py\", line 332, in spectral_layout\n",
      "    eigenvalues, eigenvectors = scipy.sparse.linalg.eigsh(\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\scipy\\sparse\\linalg\\_eigen\\arpack\\arpack.py\", line 1597, in eigsh\n",
      "    raise TypeError(\"Cannot use scipy.linalg.eigh for sparse A with \"\n",
      "TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.\n",
      "\n",
      "======================================LOGGING======================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-30 00:24:00,340]\u001b[0m Trial 7 finished with value: -100.0 and parameters: {'undersample': 'No', 'oversample': 'Yes', 'data_type': 'UMAP', 'scaling': 'Standard', 'normalizing': 'No', 'minmax': 'Yes', 'algorithm_type': 'classification', 'c_algorithm': 'XGBoost', 'data_n_neighbors': 73, 'min_dist': 0.7261172367177843, 'data_n_components': 14, 'data_metric': 'kulsinski', 'osampler': 'ADASYN', 'oversample_n_neighbors': 61}. Best is trial 3 with value: 0.5.\u001b[0m\n",
      "\u001b[32m[I 2022-11-30 00:24:00,345]\u001b[0m Trial 8 finished with value: -100.0 and parameters: {'undersample': 'Yes', 'oversample': 'Yes', 'data_type': 'RAW', 'scaling': 'None', 'normalizing': 'No', 'minmax': 'No', 'algorithm_type': 'classification', 'c_algorithm': 'Random Forest', 'usampler': 'Random', 'replacement': False, 'n_estimators': 587, 'criterion': 'gini', 'min_samples_split': 0.5285553576731011, 'min_samples_leaf': 0.11701812914742163, 'min_weight_fraction_leaf': 0.9408725304662513, 'max_features': 'log2', 'bootstrap': False, 'oob_score': True, 'class_weight': 'balanced_subsample'}. Best is trial 3 with value: 0.5.\u001b[0m\n",
      "\u001b[32m[I 2022-11-30 00:24:00,353]\u001b[0m Trial 9 finished with value: -100.0 and parameters: {'undersample': 'Yes', 'oversample': 'Yes', 'data_type': 'PCA', 'scaling': 'Standard', 'normalizing': 'Yes', 'minmax': 'Yes', 'algorithm_type': 'classification', 'c_algorithm': 'XGBoost', 'data_n_components': 72, 'whiten': False, 'svd_solver': 'randomized'}. Best is trial 3 with value: 0.5.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================LOGGING======================================\n",
      "Error is: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kelly\\AppData\\Local\\Temp\\ipykernel_17300\\885668637.py\", line 142, in optimize_dabbco\n",
      "    x_train, y_train = over_sampler.fit_resample(x_train, y_train)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\imblearn\\base.py\", line 83, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\imblearn\\over_sampling\\_adasyn.py\", line 143, in _fit_resample\n",
      "    nns = self.nn_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 749, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples,  but n_samples = 60, n_neighbors = 62\n",
      "\n",
      "======================================LOGGING======================================\n",
      "\n",
      "\n",
      "======================================LOGGING======================================\n",
      "Error is: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kelly\\AppData\\Local\\Temp\\ipykernel_17300\\885668637.py\", line 253, in optimize_dabbco\n",
      "    algo.fit(x_train, y_train)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 437, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "======================================LOGGING======================================\n",
      "\n",
      "\n",
      "======================================LOGGING======================================\n",
      "Error is: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kelly\\AppData\\Local\\Temp\\ipykernel_17300\\885668637.py\", line 38, in optimize_dabbco\n",
      "    x_train = transformer.fit_transform(mm.fit_transform(x_train))\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 433, in fit_transform\n",
      "    U, S, Vt = self._fit(X)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 485, in _fit\n",
      "    return self._fit_truncated(X, n_components, self._fit_svd_solver)\n",
      "  File \"d:\\Storage\\Personal Files\\Sinclair\\_venv\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\", line 569, in _fit_truncated\n",
      "    raise ValueError(\n",
      "ValueError: n_components=72 must be between 1 and min(n_samples, n_features)=60 with svd_solver='randomized'\n",
      "\n",
      "======================================LOGGING======================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fxn = lambda trial: optimize_dabbco(trial, X_train, X_test, y_train, y_test)\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(fxn, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all trials\n",
    "all_trials = study.get_trials()\n",
    "\n",
    "# Create a dict of the trial number and its p value\n",
    "res_trials = {}\n",
    "for _trial in all_trials:\n",
    "    res_trials[_trial.number] = (_trial.value, _trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  (0.5,\n",
       "   {'undersample': 'Yes',\n",
       "    'oversample': 'No',\n",
       "    'data_type': 'RAW',\n",
       "    'scaling': 'Robust',\n",
       "    'normalizing': 'No',\n",
       "    'minmax': 'Yes',\n",
       "    'algorithm_type': 'classification',\n",
       "    'c_algorithm': 'XGBoost',\n",
       "    'usampler': 'Repeated',\n",
       "    'undersample_n_neighbors': 12,\n",
       "    'undersample_max_iter': 50,\n",
       "    'undersample_kind_sel': 'all',\n",
       "    'max_depth': 23,\n",
       "    'min_child_weight': 45,\n",
       "    'gamma': 93,\n",
       "    'subsample': 0.9106728411908478,\n",
       "    'max_delta_step': 6})),\n",
       " (0,\n",
       "  (-100.0,\n",
       "   {'undersample': 'Yes',\n",
       "    'oversample': 'No',\n",
       "    'data_type': 'PCA',\n",
       "    'scaling': 'Standard',\n",
       "    'normalizing': 'No',\n",
       "    'minmax': 'No',\n",
       "    'algorithm_type': 'classification',\n",
       "    'c_algorithm': 'Extra Trees',\n",
       "    'data_n_components': 25,\n",
       "    'whiten': False,\n",
       "    'svd_solver': 'full',\n",
       "    'usampler': 'Random',\n",
       "    'replacement': True,\n",
       "    'n_estimators': 670,\n",
       "    'criterion': 'gini',\n",
       "    'min_samples_split': 0.6751205504194089,\n",
       "    'min_samples_leaf': 0.6770612993475804,\n",
       "    'min_weight_fraction_leaf': 0.8202993254506294,\n",
       "    'max_features': 'log2',\n",
       "    'bootstrap': False,\n",
       "    'oob_score': False,\n",
       "    'class_weight': 'balanced'})),\n",
       " (1,\n",
       "  (-100.0,\n",
       "   {'undersample': 'No',\n",
       "    'oversample': 'No',\n",
       "    'data_type': 'RAW',\n",
       "    'scaling': 'Standard',\n",
       "    'normalizing': 'Yes',\n",
       "    'minmax': 'No',\n",
       "    'algorithm_type': 'classification',\n",
       "    'c_algorithm': 'Gradient Boosting',\n",
       "    'loss': 'exponential',\n",
       "    'n_estimators': 476})),\n",
       " (2,\n",
       "  (-100.0,\n",
       "   {'undersample': 'No',\n",
       "    'oversample': 'Yes',\n",
       "    'data_type': 'RAW',\n",
       "    'scaling': 'Robust',\n",
       "    'normalizing': 'Yes',\n",
       "    'minmax': 'No',\n",
       "    'algorithm_type': 'classification',\n",
       "    'c_algorithm': 'XGBoost',\n",
       "    'osampler': 'SVMSMOTE',\n",
       "    'oversample_k_neighbors': 47,\n",
       "    'oversample_m_neighbors': 62,\n",
       "    'oversample_out_step': 0.8841320875836304,\n",
       "    'oversample_svm_c': 0.9679596527678364,\n",
       "    'oversample_svm_kernel': 'rbf',\n",
       "    'oversample_svm_gamma': 'auto'})),\n",
       " (4,\n",
       "  (-100.0,\n",
       "   {'undersample': 'Yes',\n",
       "    'oversample': 'Yes',\n",
       "    'data_type': 'RAW',\n",
       "    'scaling': 'Standard',\n",
       "    'normalizing': 'Yes',\n",
       "    'minmax': 'Yes',\n",
       "    'algorithm_type': 'classification',\n",
       "    'c_algorithm': 'Multinomial',\n",
       "    'usampler': 'Random',\n",
       "    'replacement': False})),\n",
       " (5,\n",
       "  (-100.0,\n",
       "   {'undersample': 'Yes',\n",
       "    'oversample': 'No',\n",
       "    'data_type': 'PCA',\n",
       "    'scaling': 'Robust',\n",
       "    'normalizing': 'No',\n",
       "    'minmax': 'No',\n",
       "    'algorithm_type': 'classification',\n",
       "    'c_algorithm': 'Extra Trees',\n",
       "    'data_n_components': 47,\n",
       "    'whiten': False,\n",
       "    'svd_solver': 'full',\n",
       "    'usampler': 'Repeated',\n",
       "    'undersample_n_neighbors': 86,\n",
       "    'undersample_max_iter': 81,\n",
       "    'undersample_kind_sel': 'all'})),\n",
       " (6,\n",
       "  (-100.0,\n",
       "   {'undersample': 'Yes',\n",
       "    'oversample': 'No',\n",
       "    'data_type': 'UMAP',\n",
       "    'scaling': 'Standard',\n",
       "    'normalizing': 'No',\n",
       "    'minmax': 'Yes',\n",
       "    'algorithm_type': 'classification',\n",
       "    'c_algorithm': 'Multinomial',\n",
       "    'data_n_neighbors': 60,\n",
       "    'min_dist': 0.2577743868193004,\n",
       "    'data_n_components': 63,\n",
       "    'data_metric': 'yule'})),\n",
       " (7,\n",
       "  (-100.0,\n",
       "   {'undersample': 'No',\n",
       "    'oversample': 'Yes',\n",
       "    'data_type': 'UMAP',\n",
       "    'scaling': 'Standard',\n",
       "    'normalizing': 'No',\n",
       "    'minmax': 'Yes',\n",
       "    'algorithm_type': 'classification',\n",
       "    'c_algorithm': 'XGBoost',\n",
       "    'data_n_neighbors': 73,\n",
       "    'min_dist': 0.7261172367177843,\n",
       "    'data_n_components': 14,\n",
       "    'data_metric': 'kulsinski',\n",
       "    'osampler': 'ADASYN',\n",
       "    'oversample_n_neighbors': 61})),\n",
       " (8,\n",
       "  (-100.0,\n",
       "   {'undersample': 'Yes',\n",
       "    'oversample': 'Yes',\n",
       "    'data_type': 'RAW',\n",
       "    'scaling': 'None',\n",
       "    'normalizing': 'No',\n",
       "    'minmax': 'No',\n",
       "    'algorithm_type': 'classification',\n",
       "    'c_algorithm': 'Random Forest',\n",
       "    'usampler': 'Random',\n",
       "    'replacement': False,\n",
       "    'n_estimators': 587,\n",
       "    'criterion': 'gini',\n",
       "    'min_samples_split': 0.5285553576731011,\n",
       "    'min_samples_leaf': 0.11701812914742163,\n",
       "    'min_weight_fraction_leaf': 0.9408725304662513,\n",
       "    'max_features': 'log2',\n",
       "    'bootstrap': False,\n",
       "    'oob_score': True,\n",
       "    'class_weight': 'balanced_subsample'})),\n",
       " (9,\n",
       "  (-100.0,\n",
       "   {'undersample': 'Yes',\n",
       "    'oversample': 'Yes',\n",
       "    'data_type': 'PCA',\n",
       "    'scaling': 'Standard',\n",
       "    'normalizing': 'Yes',\n",
       "    'minmax': 'Yes',\n",
       "    'algorithm_type': 'classification',\n",
       "    'c_algorithm': 'XGBoost',\n",
       "    'data_n_components': 72,\n",
       "    'whiten': False,\n",
       "    'svd_solver': 'randomized'}))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort on top 10\n",
    "parsed_trials = sorted(res_trials.items(), key=lambda x: x[1][0], reverse=True)[:25]        \n",
    "\n",
    "parsed_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put parsed trials into a dataframe for further analysis\n",
    "res_dfs = []\n",
    "for t in all_trials:\n",
    "    temp = pd.DataFrame.from_dict([t.params])\n",
    "    temp['number'] = t.number\n",
    "    temp['value'] = t.values[0] \n",
    "\n",
    "    res_dfs.append(temp)\n",
    "\n",
    "final_results = pd.concat(res_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>undersample</th>\n",
       "      <th>oversample</th>\n",
       "      <th>data_type</th>\n",
       "      <th>scaling</th>\n",
       "      <th>normalizing</th>\n",
       "      <th>minmax</th>\n",
       "      <th>algorithm_type</th>\n",
       "      <th>c_algorithm</th>\n",
       "      <th>data_n_components</th>\n",
       "      <th>whiten</th>\n",
       "      <th>...</th>\n",
       "      <th>undersample_kind_sel</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>gamma</th>\n",
       "      <th>subsample</th>\n",
       "      <th>max_delta_step</th>\n",
       "      <th>data_n_neighbors</th>\n",
       "      <th>min_dist</th>\n",
       "      <th>data_metric</th>\n",
       "      <th>oversample_n_neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>PCA</td>\n",
       "      <td>Standard</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>classification</td>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>25.0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>RAW</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RAW</td>\n",
       "      <td>Robust</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>classification</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RAW</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>classification</td>\n",
       "      <td>Multinomial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>PCA</td>\n",
       "      <td>Robust</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>classification</td>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>47.0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>all</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>Standard</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>classification</td>\n",
       "      <td>Multinomial</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.257774</td>\n",
       "      <td>yule</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>Standard</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>classification</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.726117</td>\n",
       "      <td>kulsinski</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RAW</td>\n",
       "      <td>None</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>PCA</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>classification</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>72.0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>RAW</td>\n",
       "      <td>Robust</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>classification</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>all</td>\n",
       "      <td>23.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.910673</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  undersample oversample data_type   scaling normalizing minmax  \\\n",
       "0         Yes         No       PCA  Standard          No     No   \n",
       "0          No         No       RAW  Standard         Yes     No   \n",
       "0          No        Yes       RAW    Robust         Yes     No   \n",
       "0         Yes        Yes       RAW  Standard         Yes    Yes   \n",
       "0         Yes         No       PCA    Robust          No     No   \n",
       "0         Yes         No      UMAP  Standard          No    Yes   \n",
       "0          No        Yes      UMAP  Standard          No    Yes   \n",
       "0         Yes        Yes       RAW      None          No     No   \n",
       "0         Yes        Yes       PCA  Standard         Yes    Yes   \n",
       "0         Yes         No       RAW    Robust          No    Yes   \n",
       "\n",
       "   algorithm_type        c_algorithm  data_n_components whiten  ...  \\\n",
       "0  classification        Extra Trees               25.0  False  ...   \n",
       "0  classification  Gradient Boosting                NaN    NaN  ...   \n",
       "0  classification            XGBoost                NaN    NaN  ...   \n",
       "0  classification        Multinomial                NaN    NaN  ...   \n",
       "0  classification        Extra Trees               47.0  False  ...   \n",
       "0  classification        Multinomial               63.0    NaN  ...   \n",
       "0  classification            XGBoost               14.0    NaN  ...   \n",
       "0  classification      Random Forest                NaN    NaN  ...   \n",
       "0  classification            XGBoost               72.0  False  ...   \n",
       "0  classification            XGBoost                NaN    NaN  ...   \n",
       "\n",
       "  undersample_kind_sel max_depth min_child_weight  gamma subsample  \\\n",
       "0                  NaN       NaN              NaN    NaN       NaN   \n",
       "0                  NaN       NaN              NaN    NaN       NaN   \n",
       "0                  NaN       NaN              NaN    NaN       NaN   \n",
       "0                  NaN       NaN              NaN    NaN       NaN   \n",
       "0                  all       NaN              NaN    NaN       NaN   \n",
       "0                  NaN       NaN              NaN    NaN       NaN   \n",
       "0                  NaN       NaN              NaN    NaN       NaN   \n",
       "0                  NaN       NaN              NaN    NaN       NaN   \n",
       "0                  NaN       NaN              NaN    NaN       NaN   \n",
       "0                  all      23.0             45.0   93.0  0.910673   \n",
       "\n",
       "   max_delta_step  data_n_neighbors  min_dist data_metric  \\\n",
       "0             NaN               NaN       NaN         NaN   \n",
       "0             NaN               NaN       NaN         NaN   \n",
       "0             NaN               NaN       NaN         NaN   \n",
       "0             NaN               NaN       NaN         NaN   \n",
       "0             NaN               NaN       NaN         NaN   \n",
       "0             NaN              60.0  0.257774        yule   \n",
       "0             NaN              73.0  0.726117   kulsinski   \n",
       "0             NaN               NaN       NaN         NaN   \n",
       "0             NaN               NaN       NaN         NaN   \n",
       "0             6.0               NaN       NaN         NaN   \n",
       "\n",
       "  oversample_n_neighbors  \n",
       "0                    NaN  \n",
       "0                    NaN  \n",
       "0                    NaN  \n",
       "0                    NaN  \n",
       "0                    NaN  \n",
       "0                    NaN  \n",
       "0                   61.0  \n",
       "0                    NaN  \n",
       "0                    NaN  \n",
       "0                    NaN  \n",
       "\n",
       "[10 rows x 44 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.sort_values(by='value', inplace=True)\n",
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.head(n=1).to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['clean_remarks'] = df['Remarks'].apply(process_descriptions)\n",
    "# s = pd.Series(df['clean_remarks'])\n",
    "# # Join the cleaned strings together to pass to wordcloud\n",
    "# corpus = ' '.join(s)\n",
    "\n",
    "# # Generate the wordcloud object\n",
    "# wc = WordCloud(width=500, height=250, max_words=50, normalize_plurals=True).generate(corpus)\n",
    "# plt.imshow(wc, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Most common words in Bid Remarks\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a fake $ column\n",
    "# min = 1000\n",
    "# max = 10000\n",
    "\n",
    "# df['Cost'] = [round(random.uniform(min, max), 2) for _ in range(df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in ['Priority', \"General_Contractor\", 'EST']:\n",
    "#     t = df.groupby(col).size().reset_index(name=\"Counts\").sort_values(by=\"Counts\", ascending=False)\n",
    "#     # Should we use color here?\n",
    "#     fig = px.bar(data_frame=t, x=str(col), y=\"Counts\", barmode=\"group\", color=str(col), title=\"Count of Bids by {}\".format(str(col)))\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b8df27a519a27032bb308192a175f8f3eeb140e6f2ecda7e4ddbb5cd10313e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
